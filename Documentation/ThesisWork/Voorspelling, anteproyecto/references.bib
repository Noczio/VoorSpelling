@article{Ozmen2014,
abstract = {Programming courses become prominent as one of the courses in which undergraduate students are unsuccessful especially in departments which offer computer education. Students often state that these courses are quite difficult compared to other courses. Therefore, a qualitative phenomenological approach was used to reveal the reasons of the failures of the undergraduate students in programming courses and to examine the difficulties they confronted with programming. In this scope, the laboratory practices of the Internet Programming course were observed in fall term of the 2013-2014 academic year in a university at central Anatolia. Interviews were made with 12 undergraduate students taking this course. Finally, the difficulties students experienced in the programming were determined as programming knowledge, programming skills, understanding semantics of the program, and debugging. Students emphasized that the biggest causes of failure in programming languages are lack of practice, not using algorithms and lack of knowledge. In addition, it was seen that the students who had high programming experience possess higher programming success and self-efficacy related to programming},
author = {{\"{O}}zmen, B{\"{u}}şra and Altun, Arif},
doi = {10.17569},
issn = {1309-6591},
journal = {Turkish Online Journal of Qualitative Inquiry},
number = {3},
title = {{Undergraduate Students' Experiences in Programming: Difficulties and Obstacles}},
volume = {5},
year = {2014}
}

@article{Krpan2015,
abstract = {Learning programming at university level is the challenge for both students and teachers, especially for students without previous exposure to programming. Most of the programming courses are compulsory and tough to learn for novice programmers. Students lack the understanding of basic programming concepts and algorithms and find programming difficult. Early failure of understanding important concepts weakens students' confidence and increases drop-out rate. Students' success rate and perception during most important programming courses at the undergraduate level at the Faculty of Science, University of Split over extended period of time were analyzed. Results of this research are presented in this paper.},
author = {Krpan, Divna and Mladenovi{\'{c}}, Sa{\v{s}}a and Rosi{\'{c}}, Marko},
doi = {10.1016/j.sbspro.2015.01.1126},
issn = {18770428},
journal = {Procedia - Social and Behavioral Sciences},
number = {June 2014},
pages = {3868--3872},
title = {{Undergraduate Programming Courses, Students' Perception and Success}},
volume = {174},
year = {2015}
}

@article{McCracken2001,
abstract = {In computer science, an expected outcome of a student's education is programming skill. This working group investigated the programming competency students have as they complete their first one or two courses in computer science. In order to explore options for assessing students, the working group developed a trial assessment of whether students can program. The underlying goal of this work was to initiate dialog in the Computer Science community on how to develop these types of assessments. Several universities participated in our trial assessment and the disappointing results suggest that many students do not know how to program at the conclusion of their introductory courses. For a combined sample of 216 students from four universities, the average score was 22.89 out of 110 points on the general evaluation criteria developed for this study. From this trial assessment we developed a framework of expectations for first-year courses and suggestions for further work to develop more comprehensive assessments.},
author = {McCracken, Michael and Almstrum, Vicki and Diaz, Danny and Guzdial, Mark and Hagan, Dianne and Kolikant, Yifat Ben-David and Laxer, Cary and Thomas, Lynda and Utting, Ian and Wilusz, Tadeusz},
doi = {10.1145/572139.572181},
issn = {0097-8418},
journal = {ACM SIGCSE Bulletin},
number = {4},
pages = {125--180},
title = {{A multi-national, multi-institutional study of assessment of programming skills of first-year CS students}},
volume = {33},
year = {2001}
}

@misc{PEP8Python,
title = {{PEP 8: The Style Guide for Python Code}},
url = {https://pep8.org/},
author = {Kenneth Reitz},
date = {2001-07-05},
urldate = {2020-08-30}
}

@misc{PEP257Python,
title = {{PEP 257: Docstring Conventions}},
url = {https://www.python.org/dev/peps/pep-0257/},
author = {David Goodger},
date = {2001-05-29},
urldate = {2020-08-30}
}

@misc{PEP3131Python,
title = {{PEP 3131: Supporting Non-ASCII Identifiers}},
url = {https://www.python.org/dev/peps/pep-3131/},
author = {Martin von L\"owis},
date = {2007-05-01},
urldate = {2020-09-10}
}

@misc{PEP20Python,
title = {{PEP 3131: The Zen of Python}},
url = {https://www.python.org/dev/peps/pep-0020/},
author = {Tim Peters},
date = {2004-08-19},
urldate = {2020-09-10}
}

@misc{PEP484Python,
title = {{PEP 484: Type Hints}},
url = {https://www.python.org/dev/peps/pep-0484/},
author = {Mark Shannon},
date = {2014-09-29},
urldate = {2020-09-10}
}

@misc{PEP526Python,
title = {{PEP 526: Syntax for Variable Annotations}},
url = {https://www.python.org/dev/peps/pep-0526/},
author = {Ryan Gonzalez and Philip House and Ivan Levkivskyi and Lisa Roach and Guido van Rossum},
date = {2016-08-09},
urldate = {2020-09-10}
}

@article{Tan2009,
abstract = {Researchers have been searching for alternatives in teaching programming subjects. A reason to this is due to the fact that the compulsory subject in the field of Information Technology has been a challenge and they are tough subjects to learn. On top of that, lacking the understanding in concepts has reduced undergraduates' interests to pursue further exploration and self-experimentation. In this research work, a study was conducted to investigate the factors that lead to undergraduates' learning difficulty in programming courses and also their perception on which teaching methodology could be implemented to create richer and interesting learning process. The study involved 182 undergraduates from Multimedia University, Malaysia, who have taken the fundamental programming subject named Computer Programming I. The findings affirmed that undergraduates prefer to learn programming by referring to examples and using drill-practice method whereas learning via lecturing would only decrease their interest level. The challenge has provided an evidence to call for a better solution, game-based learning as an alternative to teach and learn computer programming subjects. Therefore, the authors proposed a game-based learning framework which consists of components that leverage the pedagogical aspects in designing game-based learning environment for programming subjects. {\textcopyright} 2009 IEEE.},
author = {Tan, Phit Huan and Ting, Choo Yee and Ling, Siew Woei},
doi = {10.1109/ICCTD.2009.188},
isbn = {9780769538921},
journal = {ICCTD 2009 International Conference on Computer Technology and Development},
keywords = {Education game,Game-based learning,Interest,Learning,Motivation,Programming,Programming difficulties},
number = {May 2019},
pages = {1--5},
title = {{Learning difficulties in programming courses: Undergraduates' perspective and perception}},
volume = {1},
year = {2009}
}

@book{Francis1983,
abstract = {The MIT Press Series in Artificial Intelligence Artificial Intelligence: An MIT Perspective, Volume I: Expert Problem Solving, Natural Language Understanding, Intelligent Computer Coaches, Representation and Learning, edited by Patrick Henry Winston and Richard Henry},
author = {Francis, George K. and Abelson, Harold and DiSessa, Andrea},
booktitle = {The American Mathematical Monthly},
doi = {10.2307/2975591},
isbn = {0262010631},
issn = {00029890},
keywords = {ISBN-13:   9780262010634},
number = {6},
pages = {477},
title = {{Turtle Geometry. The Computer as a Medium for Exploring Mathematics.}},
volume = {90},
year = {1983}
}

@article{Salleh2013,
abstract = {This paper describes preliminary results of research related to programming teaching tools. This study focuses on the key issues being highlighted in this research. Among the research questions of the study are: What are the important issues in programming teaching and learning research? What are the methods of the research? What kind of tools involved in programming teaching and learning? What is the level of programming involved? The study applies systematic review approach to 45 research papers derived from the ACM digital database, published between 2005 and 2011. The study found six issues related to programming teaching tools, and most of the issues concern on the techniques and methods used in teaching, learning and assessment. Regarding the level of programming involved, majority of the research focuses on introductory stage.},
author = {Salleh, Syahanim Mohd and Shukur, Zarina and Judi, Hairulliza Mohamad},
doi = {10.1016/j.sbspro.2013.10.317},
issn = {18770428},
journal = {Procedia - Social and Behavioral Sciences},
number = {November},
pages = {127--135},
title = {{Analysis of Research in Programming Teaching Tools: An Initial Review}},
volume = {103},
year = {2013}
}

@article{Piwek2020,
abstract = {Students who study problem solving and programming (in a language such as Python) at University level encounter a range of challenges, from low-level issues with code that won't compile to misconceptions about the threshold concepts and skills. The current study complements existing findings on errors, misconceptions, difficulties and challenges obtained from students after-the-fact through instruments such as questionnaires and interviews. In our study, we analysed the posts from students of a large cohort ({\~{}}1500) of first-year University distance learning students to an online Python help forum' - recording issues and discussions as the students encountered specific challenges. Posts were coded in terms of topics, and subsequently thematically grouped into Python-related, problem solving/generic programming related, and module specific. We discuss the set of topics and rank these in terms of the number of forum discussions in which they occur (as a proxy for their prevalence). The top challenges we identified concern student understanding and use of a mix of programming environments (in particular, Python IDLE for offline programming and CodeRunner for programming quizzes) and code fragment problems. Apart from these, Python-specific topics include, among others, collections, functions, error messages, iteration, outputting results, indentation, variables and imports. We believe that the results provide a good insight into the challenges that students encounter as they learn to program. In future work we intend to study the discussions in further detail in terms of theories of conceptual change.},
author = {Piwek, Paul and Savage, Simon},
doi = {10.1145/3328778.3366838},
isbn = {9781450367936},
issn = {1942647X},
journal = {Annual Conference on Innovation and Technology in Computer Science Education, ITiCSE},
keywords = {Challenges,Misconceptions,Online student discussions,Problem solving,Programming,Python,Threshold concepts and skills},
pages = {1--6},
title = {{Challenges with learning to program and problem solve: An analysis of student online discussions}},
year = {2020}
}

@article{Thornton2013,
abstract = {Many different machine learning algorithms exist; taking into account each algorithm's hyperparameters, there is a staggeringly large number of possible alternatives overall. We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters, going beyond previous work that addresses these issues in isolation. We show that this problem can be addressed by a fully automated approach, leveraging recent innovations in Bayesian optimization. Specifically, we consider a wide range of feature selection techniques (combining 3 search and 8 evaluator methods) and all classification approaches implemented in WEKA, spanning 2 ensemble methods, 10 meta-methods, 27 base classifiers, and hyperparameter settings for each classifier. On each of 21 popular datasets from the UCI repository, the KDD Cup 09, variants of the MNIST dataset and CIFAR-10, we show classification performance often much better than using standard selection/hyperparameter optimization methods. We hope that our approach will help non-expert users to more effectively identify machine learning algorithms and hyperparameter settings appropriate to their applications, and hence to achieve improved performance.},
author = {Thornton, Chris and Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
doi = {10.1145/2487575.2487629},
keywords = {hyperparameter optimization,model selection,weka},
pages = {1--9},
title = {{Auto-WEKA}},
year = {2013}
}

@article{Hall2009,
abstract = {Abstract More than twelve years have elapsed since the first public release of WEKA . In that time, the software has been rewritten entirely from scratch, evolved substantially and now accompanies a text on data mining [35]. These days, WEKA enjoys widespread ... $\backslash$n},
author = {Hall, Mark and Frank, Eibe and Holmes, Geoffrey and Pfahringer, Bernhard and Reutemann, Peter and Witten, Ian H.},
doi = {10.1145/1656274.1656278},
issn = {1931-0145},
journal = {ACM SIGKDD Explorations Newsletter},
number = {1},
pages = {10--18},
title = {{The WEKA data mining software}},
volume = {11},
year = {2009}
}

@book{Ethem2014,
author = {{Ethem Alpaydin}},
booktitle = {Journal of Visual Languages \& Computing},
isbn = {9786162833052},
edition = {3},
pages = {613},
title = {{Introduction to Machine Learning}},
url = {https://mitpress.mit.edu/books/introduction-machine-learning-third-edition},
year = {2014}
}

@article{Bost2015,
abstract = {Machine learning classification is used for numer- ous tasks nowadays, such as medical or genomics predictions, spam detection, face recognition, and financial predictions. Due to privacy concerns, in some of these applications, it is important that the data and the classifier remain confidential. In this work, we construct three major classification protocols that satisfy this privacy constraint: hyperplane decision, Naive Bayes, and decision trees. We also enable these protocols to be combined with AdaBoost. At the basis of these constructions is a new library of building blocks, which enables constructing a wide range of privacy-preserving classifiers; we demonstrate how this library can be used to construct other classifiers than the three mentioned above, such as a multiplexer and a face detection classifier. We implemented and evaluated our library and our classifiers. Our protocols are efficient, taking milliseconds to a few seconds to perform a classification when running on real medical datasets.},
author = {Bost, Raphael and Popa, Raluca Ada and Tu, Stephen and Goldwasser, Shafi},
doi = {10.14722/ndss.2015.23241},
isbn = {189156238X},
number = {February},
pages = {1-14},
title = {{Machine Learning Classification over Encrypted Data}},
year = {2015}
}

@book{Han2012,
abstract = {This is the third edition of the premier professional reference on the subject of data mining, expanding and updating the previous market leading edition. This was the first (and is still the best and most popular) of its kind. Combines sound theory with truly practical applications to prepare students for real-world challenges in data mining. Like the first and second editions, Data Mining: Concepts and Techniques, 3rd Edition equips professionals with a sound understanding of data mining principles and teaches proven methods for knowledge discovery in large corporate databases. The first and second editions also established itself as the market leader for courses in data mining, data analytics, and knowledge discovery. Revisions incorporate input from instructors, changes in the field, and new and important topics such as data warehouse and data cube technology, mining stream data, mining social networks, and mining spatial, multimedia and other complex data. This book begins with a conceptual introduction followed by a comprehensive and state-of-the-art coverage of concepts and techniques. Each chapter is a stand-alone guide to a critical topic, presenting proven algorithms and sound implementations ready to be used directly or with strategic modification against live data. Wherever possible, the authors raise and answer questions of utility, feasibility, optimization, and scalability. relational data. -- A comprehensive, practical look at the concepts and techniques you need to get the most out of real business data. -- Updates that incorporate input from readers, changes in the field, and more material on statistics and machine learning, -- Scores of algorithms and implementation examples, all in easily understood pseudo-code and suitable for use in real-world, large-scale data mining projects. -- Complete classroom support for instructors as well as bonus content available at the companion website. A comprehensive and practical look at the concepts and techniques you need in the area of data mining and knowledge discovery. {\textcopyright} 2012 Elsevier Inc. All rights reserved.},
author = {Han, Jiawei and Kamber, Micheline and Pei, Jian},
booktitle = {Data Mining: Concepts and Techniques},
doi = {10.1016/C2009-0-61819-5},
isbn = {9780123814791},
publisher = {Elsevier Inc.},
title = {{Data Mining: Concepts and Techniques}},
year = {2012}
}

@article{Finley2005,
abstract = {Supervised clustering is the problem of training a clustering algorithm to produce desirable clusterings: given sets of items and complete clusterings over these sets, we learn how to cluster future sets of items. Example applications include noun-phrase coreference clustering, and clustering news articles by whether they refer to the same topic. In this paper we present an SVM algorithm that trains a clustering algorithm by adapting the item-pair similarity measure. The algorithm may optimize a variety of different clustering functions to a variety of clustering performance measures. We empirically evaluate the algorithm for noun-phrase and news article clustering.},
author = {Finley, Thomas and Joachims, Thorsten},
doi = {10.1145/1102351.1102379},
isbn = {1595931805},
journal = {ICML 2005 - Proceedings of the 22nd International Conference on Machine Learning},
pages = {217--224},
title = {{Supervised clustering with support vector machines}},
year = {2005}
}


@book{Russell2003,
author = {Russell, S. and Norvig, Peter},
title = {{Artificial intelligence - a modern approach}},
edition = {2},
year = {2003},
isbn = {978-0137903955}
}

@article{Guerrero2016,
abstract = {El problema de la dimensionalidad lo definiremos como los potenciales efectos negativos derivados del aumento del n{\'{u}}mero de variables respecto a las observaciones.},
author = {Guerrero, Jos{\'{e}} A.},
journal = {Revista de Estad{\'{i}}stica y Sociedad},
keywords = {Causality,General causality test of causation,causalidad general,criminal law,criminal procedure/Causalidad,derecho penal,proceso penal,prueba de la causalidad},
number = {68},
pages = {22--24},
title = {{El problema de la dimensionalidad}},
url = {https://dialnet.unirioja.es/servlet/articulo?codigo=5711139},
volume = {1},
year = {2016}
}

@inproceedings{Sanchez-Marono2007,
abstract = {Adequate selection of features may improve accuracy and efficiency of classifier methods. There are two main approaches for feature selection: wrapper methods, in which the features are selected using the classifier, and filter methods, in which the selection of features is independent of the classifier used. Although the wrapper approach may obtain better performances, it requires greater computational resources. For this reason, lately a new paradigm, hybrid approach, that combines both filter and wrapper methods has emerged. One of its problems is to select the filter method that gives the best relevance index for each case, and this is not an easy to solve question. Different approaches to relevance evaluation lead to a large number of indices for ranking and selection. In this paper, several filter methods are applied over artificial data sets with different number of relevant features, level of noise in the output, interaction between features and increasing number of samples. The results obtained for the four filters studied (ReliefF, Correlation-based Feature Selection, Fast Correlated Based Filter and INTERACT) are compared and discussed. The final aim of this study is to select a filter to construct a hybrid method for feature selection. {\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
author = {S{\'{a}}nchez-Maro{\~{n}}o, Noelia and Alonso-Betanzos, Amparo and Tombilla-Sanrom{\'{a}}n, Mar{\'{i}}a},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-77226-2_19},
isbn = {9783540772255},
issn = {16113349},
month = {12},
pages = {178--187},
publisher = {Springer Verlag},
title = {{Filter methods for feature selection - A comparative study}},
url = {https://link.springer.com/chapter/10.1007/978-3-540-77226-2{\_}19},
volume = {4881 LNCS},
year = {2007}
}



@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@inproceedings{sklearn_api,
  author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
               Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
               Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
               and Jaques Grobler and Robert Layton and Jake VanderPlas and
               Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
  title     = {{API} design for machine learning software: experiences from the scikit-learn
               project},
  booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
  year      = {2013},
  pages = {108--122},
}

@article{Khan2014,
abstract = {This thesis concentr ates on improving the existing a gile software development process used in the case company in order to overcome the various problems faced during the product development based on the existing Scrum model. The case company , consisting of sixty five em ployees, turns any online content, images, videos and applications into i n- teractive and viral storefronts by means of non - intrusive and content relevant ad vertis e- ment products . After describing the research objective, researcher started with the current state analysis of the agile scrum process within the Applications Development team of the case comp a- ny . Qualitative research methodology was utilized in th is study. The data for current anal y- sis was collected based on interview discussions, analysing past sprint retrospectives, and sprint velocity data. The participants used for interviews and discussions were from various backgrounds and departments of the case company and include d experts such as, Scrum Mas ter , Product Own er , Product Manag er , Quality Assurance Team , Team Manager , O p- eration Man ager and Software Developers . Overall eight (8) persons were interviewed. Based on current state analysis and litera ture review, a new process model was defined by the researcher. This research model was further improved after gathering qualitative r e- search data from different stake holders and experts mentioned earlier. The new proposed process model was further pilot tested in the case company for four itera tions. The results and final conclusion from the piloted iterations were further documented in this study. The results from the pilot testing indicated that the new process model has provided the needed flexibilit y to the team and product owners, work flow across different stages of pr o- cess has improved, team collaboration has improved and in addition team is constantly improving their work policies},
author = {Khan, Zahoor},
keywords = {Agile Scrum methodology,Kanban,Product  Development process,Scrumban},
number = {May},
pages = {122},
title = {{Scrumban-Adaptive Agile Development Process: Using scrumban to improve software development process}},
url = {http://urn.fi/URN:NBN:fi:amk-2014052710211},
year = {2014}
}

@book{Pressman2002,
abstract = {applicability for this approach.},
author = {Pressman, R.},
edition = {7},
isbn = {9786071503145},
publisher = {McGraw-Hill},
title = {{Ingenier{\'{i}}a del Software. Un enfoque pr{\'{a}}ctico.}},
year = {2002}
}

@misc{Iso11581,
title = {{ISO/IEC 11581-10}},
url = {https://www.iso.org/standard/46445.html},
year = {2010},
urldate = {2020-10-03}
}

@misc{Iso9241-112,
title = {{ISO 9241-112}},
url = {https://www.iso.org/standard/64840.html},
year = {2017},
urldate = {2020-10-10}
}

@misc{Iso9241-210,
title = {{ISO 9241-210}},
url = {https://www.iso.org/standard/77520.html},
year = {2019},
urldate = {2020-10-03}
}

@misc{Auger,
title = {{Auger.AI}},
url = {https://auger.ai},
urldate = {2020-10-29}
}

@article{Brochu2010,
abstract = {We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.},
archivePrefix = {arXiv},
arxivId = {1012.2599},
author = {Brochu, Eric and Cora, Vlad M. and de Freitas, Nando},
eprint = {1012.2599},
month = {12},
pages = {1--49},
title = {{A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning}},
url = {http://arxiv.org/abs/1012.2599},
year = {2010}
}

@article{Wu2019,
abstract = {Hyperparameters are important for machine learning algorithms since they directly control the behaviors of training algorithms and have a significant effect on the performance of machine learning models. Several techniques have been developed and successfully applied for certain application domains. However, this work demands professional knowledge and expert experience. And sometimes it has to resort to the brute-force search. Therefore, if an efficient hyperparameter optimization algorithm can be developed to optimize any given machine learning method, it will greatly improve the efficiency of machine learning. In this paper, we consider building the relationship between the performance of the machine learning models and their hyperparameters by Gaussian processes. In this way, the hyperparameter tuning problem can be abstracted as an optimization problem and Bayesian optimization is used to solve the problem. Bayesian optimization is based on the Bayesian theorem. It sets a prior over the optimization function and gathers the information from the previous sample to update the posterior of the optimization function. A utility function selects the next sample point to maximize the optimization function. Several experiments were conducted on standard test datasets. Experiment results show that the proposed method can find the best hyperparameters for the widely used machine learning models, such as the random forest algorithm and the neural networks, even multi-grained cascade forest under the consideration of time cost.},
author = {Wu, Jia and Chen, Xiu Yun and Zhang, Hao and Xiong, Li Dong and Lei, Hang and Deng, Si Hao},
doi = {10.11989/JEST.1674-862X.80904120},
issn = {1674862X},
journal = {Journal of Electronic Science and Technology},
keywords = {Bayesian optimization,Gaussian process,Hyperparameter optimization,Machine learning},
month = {3},
number = {1},
pages = {26--40},
publisher = {University of Electronic Science and Technology of China},
title = {{Hyperparameter optimization for machine learning models based on Bayesian optimization}},
volume = {17},
year = {2019}
}

@book{Python3,
 author = {Van Rossum, Guido and Drake, Fred L.},
 title = {Python 3 Reference Manual},
 year = {2009},
 isbn = {1441412697},
 publisher = {CreateSpace},
 address = {Scotts Valley, CA}
}

@misc{Pythondoc,
title = {{Python 3.7 documentation}},
url = {https://docs.python.org/3.7/},
year = {2018},
urldate = {2020-10-06}
}

@misc{QTDesDoc,
title = {{QT Designer Manual}},
url = {https://doc.qt.io/qt-5/qtdesigner-manual.html},
year = {2020},
urldate = {2020-10-06}
}

@misc{QTDes,
author = {Haavard Nord and Eirik Chambe-Eng},
title = {{QT Designer}},
url = {https://www.qt.io/},
year = {1995},
urldate = {2020-10-06}
}

@misc{CambridgeDefFramework,
author = {{Cambridge Dictionary}},
title = {{Meaning of framework}},
url = {https://dictionary.cambridge.org/us/dictionary/english/framework},
urldate = {2020-10-07}
}

@misc{RAEDefNorma,
author = {{Diccionario de la lengua espa{\~{n}}ola | RAE}},
title = {{Norma | Definici{\'{o}}n}},
url = {https://dle.rae.es/norma},
urldate = {2020-10-10}
}

@misc{RAEDefDocumentacion,
author = {{Diccionario de la lengua espa{\~{n}}ola | RAE}},
title = {{Documentaci{\'{o}}n | Definici{\'{o}}n}},
url = {https://dle.rae.es/documentaci{\'{o}}n},
urldate = {2020-10-15}
}

@misc{CambridgeDefMethodology,
author = {{Cambridge Dictionary}},
title = {{Meaning of methodology}},
url = {https://dictionary.cambridge.org/us/dictionary/english/methodology},
urldate = {2020-10-19}
}

@book{Beck2004,
abstract = {Wow—the second edition. I cannot believe that five years have already passed since the appearance of the first edition. When Kent pinged me to write a foreword to the second edition I asked him for a manuscript version with change bars. What a silly request—the book is a full rewrite! In the second edition of XP Explained Kent revisits XP and applies the XP paradigm—stay aware, adapt, change—to XP itself. Kent has revisited, cleaned-up, and refactored every bit of XP Explained and integrated many new insights. The result is XP Explained even better explained! This is an excellent opportunity to reflect on how XP has influence. my own software development. Shortly after the first edition of XP Explained I became involved in the Eclipse project and it is now absorbing all my software energy. Eclipse isn't run under the pure XP flag. We follow agile practices; however, the XP influences are easy to spot. The most obvious one is that we have encoded several XP prac- tices directly into our tool. Refactoring, unit testing, and immediate feedback as you code are now an integral part of our toolset. Moreover, since we are “eating our own dog food” we use these practices in our day-to-day development. Even more interesting are the XP influences one can spot in our development process. Eclipse is an open source project and one of our goals is to practice completely transparent devel- opment. The rationale is simple; if you don't know where the project is going you cannot help out or provide feedback. XP practices help us to achieve this goal.},
author = {{Kent Beck}},
edition = {2},
editor = {Mulcahy, Julie Nahil and Kim Arney},
isbn = {0321278658},
pages = {189},
publisher = {John Wait},
title = {{Extreme Programming Explained}},
year = {2004}
}

@Book{raschka2015python,
 author = {{Sebastian Raschka}},
 title = {Python Machine Learning},
 publisher = {Packt Publishing},
 edition = {3},
 year = {2019},
 address = {Birmingham, UK},
 isbn = {9781789955750}
 }
 
 @Book{oswald2020python,
 author = {{Oswald Campesato}},
 title = {Python 3 for Machine Learning},
 publisher = {Mercury Learning and Information},
 year = {2020},
 address = {22841 Quicksilver Drive},
 isbn = {9781683924951}
 }
 
 @book{Geron2019,
abstract = {Graphics in this book are printed in black and white. Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how. By using concrete examples, minimal theory, and two production-ready Python framework sscikit-learn and Tensor Flow helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. Youll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what youve learned, all you need is programming experience to get started. Explore the machine learning landscape, particularly neural nets Use scikit-learn to track an example machine-learning project end-to-end Explore several training models, including support vector machines, decision trees, random forests, and ensemble methods Use the Tensor Flow library to build and train neural nets Dive into neural net architectures, including convolutional nets, recurrent nets, and deep reinforcement learning Learn techniques for training and scaling deep neural nets Apply practical code examples without acquiring excessive machine learning theory or algorithm details},
author = {G{\'{e}}ron, Aur{\'{e}}lien},
edition = {2},
editor = {Tache, Nicole},
isbn = {978-1492032649},
pages = {510},
publisher = {O'Reilly Media, Inc},
title = {{Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow}},
year = {2019}
}
 
 @book{Rumbaugh2004,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
author = {Rumbaugh, James and Jacobson, Ivar and Booch, Grady},
edition = {2},
isbn = {0-321-24562-8},
pages = {742},
publisher = {Addison-Wesley Longman, Inc},
title = {{The Unified Modeling Language Reference Manual}},
year = {2004}
}
 
@misc{mljar2018,
author = {Piotr and shahules786 and spamz23 and abtheo and uditswaroopa and partrit},
title = {{mljar-supervised}},
url = {https://github.com/mljar/mljar-supervised},
date = {2018-11-04},
urldate = {2020-10-10}
}

@article{Feurer2020,
abstract = {Automated Machine Learning, which supports practitioners and researchers with the tedious task of manually designing machine learning pipelines, has recently achieved substantial success. In this paper we introduce new Automated Machine Learning (AutoML) techniques motivated by our winning submission to the second ChaLearn AutoML challenge, PoSH Auto-sklearn. For this, we extend Auto-sklearn with a new, simpler meta-learning technique, improve its way of handling iterative algorithms and enhance it with a successful bandit strategy for budget allocation. Furthermore, we go one step further and study the design space of AutoML itself and propose a solution towards truly hand-free AutoML. Together, these changes give rise to the next generation of our AutoML system, Auto-sklearn (2.0). We verify the improvement by these additions in a large experimental study on 39 AutoML benchmark datasets and conclude the paper by comparing to Auto-sklearn (1.0), reducing the regret by up to a factor of five.},
archivePrefix = {arXiv},
arxivId = {2007.04074},
author = {Feurer, Matthias and Eggensperger, Katharina and Falkner, Stefan and Lindauer, Marius and Hutter, Frank},
eprint = {2007.04074},
keywords = {Automated Machine Learning,Hyperparameter Optimization,Meta-Learning},
pages = {1--18},
title = {{Auto-Sklearn 2.0: The Next Generation}},
url = {http://arxiv.org/abs/2007.04074},
year = {2020}
}

@book{Sommerville2005,
author = {Sommerville, Ian and Mar{\'{i}}a, Traducci{\'{o}}n and Galipienso, Isabel Alfonso and Bot{\'{i}}a, Antonio and Francisco, Mart{\'{i}}nez and Liz{\'{a}}n, Mora and Pascua, Jos{\'{e}} and Jover, Trigueros},
edition = {S{\'{e}}ptima ed},
editor = {Mart{\'{i}}n-Romo, Miguel},
file = {:C$\backslash$:/Users/GERMAN/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Edici{\'{o}}n et al. - 2005 - Ingenier{\'{i}}a del software.pdf:pdf},
isbn = {8478290745},
pages = {712},
publisher = {PEARSON EDUCACI{\'{O}}N, S.A.},
title = {{Ingenier{\'{i}}a del software}},
year = {2005}
}

@article{Demsar2004,
abstract = {Orange (www.ailab.si/orange) is a suite for machine learning and data mining. For researchers in machine learning, Orange offers scripting to easily prototype new algorithms and experimental procedures. For explorative data analysis, it provides a visual programming framework with emphasis on interactions and creative combinations of visual components. {\textcopyright} Springer-Verlag Berlin Heidelberg 2004.},
author = {Dem{\v{s}}ar, Janez and Zupan, Bla{\v{z}} and Leban, Gregor and Curk, Tomaz},
doi = {10.1007/978-3-540-30116-5_58},
isbn = {3540231080},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {August 2014},
pages = {537--539},
title = {{Orange: From experimental machine learning to interactive data mining}},
volume = {3202},
year = {2004}
}

@INPROCEEDINGS{BCDG07,
  author = {Michael R. Berthold and Nicolas Cebron and Fabian Dill and Thomas R. Gabriel and Tobias K\"{o}tter and Thorsten Meinl and Peter Ohl and Christoph Sieb and Kilian Thiel and Bernd Wiswedel},
  title = {{KNIME}: The {K}onstanz {I}nformation {M}iner},
  booktitle = {Studies in Classification, Data Analysis, and Knowledge Organization (GfKL 2007)},
  publisher = {Springer},
  ISBN = {978-3-540-78239-1},
  ISSN = {1431-8814},
  year = {2007}
}

@misc{googleML,
author = {Google},
title = {{Google AI Platform}},
url = {https://cloud.google.com/ai-platform},
year = {2018},
urldate = {2020-10-29}
}


